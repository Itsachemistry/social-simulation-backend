"""
æµ‹è¯•å¸¦çœŸå®LLMè°ƒç”¨çš„å®æ—¶ç›‘æ§åŠŸèƒ½
"""
import json
import time
import os
from src.main import SimulationEngine

def test_realtime_monitoring_with_llm():
    """æµ‹è¯•å¸¦çœŸå®LLMè°ƒç”¨çš„å®æ—¶ç›‘æ§åŠŸèƒ½"""
    print("=== æµ‹è¯•å¸¦çœŸå®LLMè°ƒç”¨çš„å®æ—¶ç›‘æ§åŠŸèƒ½ ===")
    
    # å‡†å¤‡é…ç½®ï¼Œä½¿ç”¨çœŸå®çš„DeepSeek API
    config = {
        "max_slices": 2,  # åªè¿è¡Œ2ä¸ªæ—¶é—´ç‰‡è¿›è¡Œæµ‹è¯•
        "llm_config": {
            "api_key": "sk-oPCS3RtcJEtOORaFvskbBI75eJ6jzJcs4vtK3I2vyw7DcrRK",
            "base_url": "https://www.chataiapi.com/v1/chat/completions",
            "model": "gpt-3.5-turbo",
            "enabled": True  # å¯ç”¨çœŸå®LLM
        },
        "agents": [
            {
                "id": "user1",
                "personality": "ç§¯æä¹è§‚çš„å¹´è½»äºº",
                "is_opinion_leader": False,
                "initial_emotion": 0.7,
                "initial_stance": 0.6
            },
            {
                "id": "leader1", 
                "personality": "ç†æ€§åˆ†æçš„æ„è§é¢†è¢–",
                "is_opinion_leader": True,
                "initial_emotion": 0.5,
                "initial_stance": 0.3
            }
        ]
    }
    
    # åˆ›å»ºä»¿çœŸå¼•æ“
    engine = SimulationEngine(config)
    print(f"Agentæ•°é‡: {len(engine.agent_controller.agents)}")
    
    # æ³¨å…¥ä¸€ä¸ªçƒ­é—¨äº‹ä»¶æ¥è§¦å‘LLMè°ƒç”¨
    print("æ³¨å…¥æµ‹è¯•äº‹ä»¶...")
    event_id = engine.inject_event("æœ€æ–°ç§‘æŠ€æ–°é—»ï¼šAIæŠ€æœ¯å–å¾—é‡å¤§çªç ´", event_heat=90)
    
    # å¼€å§‹ä»¿çœŸï¼ˆè¿™ä¼šåˆ›å»ºå®æ—¶æ—¥å¿—æ–‡ä»¶å¹¶åŒ…å«LLMè°ƒç”¨ï¼‰
    print("\nå¼€å§‹ä»¿çœŸï¼Œå®æ—¶ç›‘æ§å°†æ•è·æ‰€æœ‰LLM Promptå’Œå“åº”...")
    start_time = time.time()
    
    results = engine.run_simulation(max_slices=2)
    
    elapsed = time.time() - start_time
    print(f"\næµ‹è¯•å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
    
    # æ£€æŸ¥ç”Ÿæˆçš„æ—¥å¿—æ–‡ä»¶
    import glob
    log_files = glob.glob("simulation_log_*.txt")
    if log_files:
        latest_log = max(log_files, key=os.path.getctime)
        print(f"æœ€æ–°æ—¥å¿—æ–‡ä»¶: {latest_log}")
        
        # åˆ†ææ—¥å¿—å†…å®¹
        print("\n=== æ—¥å¿—å†…å®¹åˆ†æ ===")
        with open(latest_log, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ç»Ÿè®¡å…³é”®ä¿¡æ¯
        llm_calls = content.count("[LLM Request]")
        llm_responses = content.count("[LLM Response]")
        agent_updates = content.count("Agent")
        leader_updates = content.count("[Leader]")
        
        print(f"ğŸ“Š ç›‘æ§ç»Ÿè®¡:")
        print(f"  - LLMè¯·æ±‚æ¬¡æ•°: {llm_calls}")
        print(f"  - LLMå“åº”æ¬¡æ•°: {llm_responses}")
        print(f"  - AgentçŠ¶æ€æ›´æ–°: {agent_updates}")
        print(f"  - æ„è§é¢†è¢–æ›´æ–°: {leader_updates}")
        print(f"  - æ€»æ—¥å¿—è¡Œæ•°: {len(content.splitlines())}")
        
        # æ˜¾ç¤ºåŒ…å«LLMè°ƒç”¨çš„å…³é”®è¡Œ
        lines = content.splitlines()
        print(f"\n=== LLMè°ƒç”¨ç›‘æ§æ ·ä¾‹ ===")
        for i, line in enumerate(lines):
            if "[LLM" in line:
                print(f"ç¬¬{i+1}è¡Œ: {line}")
                # æ˜¾ç¤ºä¸‹ä¸€è¡Œçš„å†…å®¹ï¼ˆé€šå¸¸æ˜¯promptæˆ–responseå†…å®¹ï¼‰
                if i+1 < len(lines):
                    next_line = lines[i+1].strip()
                    if next_line:
                        print(f"å†…å®¹: {next_line[:100]}...")
                print()
        
        # æä¾›æŸ¥çœ‹å®Œæ•´æ—¥å¿—çš„æç¤º
        print(f"\nğŸ’¡ æŸ¥çœ‹å®Œæ•´å®æ—¶æ—¥å¿—è¯·æ‰“å¼€: {latest_log}")
        print("   è¯¥æ–‡ä»¶åŒ…å«äº†ä»¿çœŸè¿‡ç¨‹ä¸­çš„æ‰€æœ‰LLM Promptã€å“åº”å’ŒAgentçŠ¶æ€å˜åŒ–")
        
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
    
    return True

if __name__ == "__main__":
    test_realtime_monitoring_with_llm()
